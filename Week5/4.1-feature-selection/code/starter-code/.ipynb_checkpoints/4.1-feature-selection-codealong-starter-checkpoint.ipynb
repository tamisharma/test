{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection lesson/lab\n",
    "\n",
    "In this codealong we will explore different ways of performing **feature selection**.\n",
    "\n",
    "Feature selection is the process of reducing the number of predictors in your data based on their calculated \"usefulness\". This is the flip side of the process of **feature engineering**, where you create new descriptive predictors from the data you already have.\n",
    "\n",
    "You have already had experience with the **Lasso**, which (in my opinion), is the best feature selector despite it's downsides. [ASIDE: The downsides of the Lasso are in fact addressed by the \"Elastic Net\" penalty, which combines the Ridge regularization with the Lasso regularization!]\n",
    "\n",
    "In the first half we will be revisiting and practicing using the Lasso in a classification task of identifying spam text messages from the appearance of particular words in the text message. This dataset has been created with the `CountVectorizer` that you are familiar with from previous lessons, though you will not be using it here in the interest of time.\n",
    "\n",
    "The second half you will explore, as groups, alternative methods of doing feature selection provided in the scikit-learn package and then presenting on how they work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv('../../assets/datasets/spam_words_wide.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Find baseline rate of spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1 Cross-validate logistic regression accuracy\n",
    "\n",
    "Use these classes/methods from scikit-learn:\n",
    "\n",
    "    LogisticRegression\n",
    "    cross_val_score\n",
    "    \n",
    "Cross-validate the logistic regression with 5 folds.\n",
    "\n",
    "Also save the X matrix column headers for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Cross-validate logistic regression area under ROC\n",
    "\n",
    "The `scoring` keyword argument in `cross_val_score()` can take different scoring metrics than the default \"accuracy\".\n",
    "\n",
    "For more information on how to use this [read the documentation on model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html) particularly in **Section 3.3.1**.\n",
    "\n",
    "Why is using the area under the ROC curve more informative than the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Cross-validate the logistic regression, scoring on the area under the precision-recall curve\n",
    "\n",
    "The \"average_precision\" is the area under the precision-recall curve, whereas \"precision\" is simply the precision without taking recall into consideration.\n",
    "\n",
    "Why/when might you decide to use precision and recall in a classification task (area under precision-recal curve) versus specificity and sensitivity (area under the ROC curve)?\n",
    "\n",
    "These wikipedia pages are good references for refreshing your memory or figuring out the differences and goals of each:\n",
    "\n",
    "1. [Confusion matrices](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "2. [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "3. [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "4. [Type I and Type II errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Fit the logistic regression on all the data and calculate the number of non-negative coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1 Cross-validate logreg Lasso regularization using the 'average_precision' scoring metric\n",
    "\n",
    "Use the sklearn class:\n",
    "\n",
    "    LogisticRegressionCV\n",
    "    \n",
    "Remember that these keyword arguments need to be used for the Lasso:\n",
    "\n",
    "    solver='liblinear'\n",
    "    penalty='l1' (That is a lowercase 'L' first!)\n",
    "    \n",
    "Cross-validate with 25 folds and print out the best regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2 Build a logistic regression using Lasso penalty with the optimal C and cross-validate area under the precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3 Find how many non-zero coefficients there are for this model\n",
    "\n",
    "How does this compare to the non-Lasso model? Explain why the Lasso performs \"feature selection\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.4 Repeat this process but with scoring='precision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_lasso_cv = LogisticRegressionCV(solver='liblinear', cv=5, penalty='l1', Cs=25,\n",
    "                                   scoring='precision')\n",
    "\n",
    "lr_lasso_cv.fit(X, Y)\n",
    "\n",
    "print lr_lasso_cv.C_\n",
    "\n",
    "lr_lasso = LogisticRegression(solver='liblinear', penalty='l1', C=lr_lasso_cv.C_[0])\n",
    "\n",
    "scores = cross_val_score(lr_lasso, X, Y, cv=5, scoring='precision')\n",
    "print np.mean(scores)\n",
    "\n",
    "lr_lasso.fit(X, Y)\n",
    "\n",
    "print np.sum(lr_lasso.coef_[0] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.5 Use the X column names and the coefficients from the logistic regression to find out which features were kept when scoring for precision with the Lasso penalty\n",
    "\n",
    "Explain what these chosen features are useful for when scoring for optimal precision.\n",
    "\n",
    "Why are there so few compared to the area under the precision-recall curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Explore other feature selection methods\n",
    "\n",
    "scikit-learn comes with [a variety of other feature selection methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection).\n",
    "\n",
    "For the next section you will explore as groups these methods:\n",
    "\n",
    "**Group 1**\n",
    "    \n",
    "    feature_selection.SelectPercentile\n",
    "    feature_selection.SelectKBest\n",
    "    \n",
    "**Group 2**\n",
    "    \n",
    "    feature_selection.RFE\n",
    "    feature_selection.RFECV\n",
    "    \n",
    "**Group 3**\n",
    "    \n",
    "    feature_selection.SelectFpr\n",
    "    feature_selection.SelectFdr\n",
    "    \n",
    "**Group 4**\n",
    "\n",
    "    feature_selection.VarianceThreshold\n",
    "    feature_selection.SelectFwe\n",
    "    \n",
    "---\n",
    "\n",
    "#### Questions for presentation\n",
    "\n",
    "After exploring the assigned feature selection methods, you will, as a group, present to the class on:\n",
    "\n",
    "1. How the feature selection method is designed to reduce the number of predictors.\n",
    "2. What scenario(s) you think the method would be particularly useful in.\n",
    "3. How to implement the method in code.\n",
    "4. [BONUS] Possible downsides to using the feature selection method (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
